"""Trains the model on data/final_housing_data.csv
generated by data_processing.py
"""
import os
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
import joblib

def load_and_prepare_data(file_path):
    """Load the housing data and prepare features and target.

    Args:
        file_path: str. The path to the CSV file containing the data.

    Returns:
        A tuple (features_df, target_series) where:
        - features_df: pd.DataFrame. The feature columns.
        - target_series: pd.Series. The target values with NaNs removed.
    """
    data_df = pd.read_csv(file_path)
    features_df = data_df[['median_salary', 'population_size', 'number_of_jobs', 'area_size']]
    target_series = pd.to_numeric(data_df['mean_salary'], errors='coerce').dropna()
    features_df = features_df.loc[target_series.index]  # Align features with the cleaned target

    return features_df, target_series

def train_random_forest_model(x_train, y_train, random_state=42):
    """Train a RandomForestRegressor model.

    Args:
        x_train: np.ndarray. The training feature data.
        y_train: pd.Series. The training target values.
        random_state: int. The random state for reproducibility.

    Returns:
        A trained RandomForestRegressor model.
    """
    model = RandomForestRegressor(random_state=random_state)
    model.fit(x_train, y_train)
    return model

def save_model_and_scaler(model, scaler, model_file_path, scaler_file_path):
    """Save the trained model and scaler to disk.

    Args:
        model: The trained model.
        scaler: The feature scaler.
        model_file_path: str. Path to save the trained model.
        scaler_file_path: str. Path to save the scaler.

    Returns:
        None
    """
    os.makedirs(os.path.dirname(model_file_path), exist_ok=True)
    joblib.dump(model, model_file_path)
    joblib.dump(scaler, scaler_file_path)

def main():
    """Main function to load data, train the model, and save the model and scaler."""
    # Load the dataset
    features_df, target_series = load_and_prepare_data('data/final_housing_data.csv')

    # Split data into training sets
    x_train, x_test, y_train, y_test = train_test_split(
        features_df, target_series, test_size=0.2, random_state=42
    )

    # Scale the features
    scaler = StandardScaler()
    x_train_scaled = scaler.fit_transform(x_train)

    # Train and save the model
    model = train_random_forest_model(x_train_scaled, y_train)
    save_model_and_scaler(model, scaler,
                          'models/housing_price_model.pkl',
                          'models/housing_scaler.pkl')

if __name__ == '__main__':
    main()
